{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing general plots\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#taking out data from train.csv\n",
    "dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checking out dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shape of dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#Downloading stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing all the numbers and characters other than a to z and A to Z\n",
    "review = re.sub('[^a-zA-Z]' , ' ' , dataset['comment_text'][0])\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(0 , 95851):\n",
    "#     #Removing all the numbers and characters other than a to z and A to Z\n",
    "#     review = re.sub('[^a-zA-Z]' , ' ' , dataset['comment_text'][i])\n",
    "#     #Converting into lower case\n",
    "#     review = review.lower()\n",
    "#     #Splitting\n",
    "#     review = review.split()\n",
    "#     #Appying Stemmer \n",
    "#     ps = PorterStemmer()\n",
    "#     review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "#     # Again join with space to form sentence\n",
    "#     review = ' '.join(review)\n",
    "#     #Appending to corpus list\n",
    "#     #corpus.append(review)\n",
    "#     dataset['comment_text'][i] = review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.lower())\n",
    "dataset['comment_text'] = dataset['comment_text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2000, split=' ')\n",
    "tokenizer.fit_on_texts(dataset['comment_text'].values)\n",
    "X = tokenizer.texts_to_sequences(dataset['comment_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "max_features = 2000\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(6,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = dataset.iloc[ : , [2,3,4,5,6,7]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, nb_epoch = 100, batch_size=batch_size, verbose = 2)\n",
    "model.save_weights(\"Weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## New Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (95851, 7)\n"
     ]
    }
   ],
   "source": [
    "def ingest():\n",
    "    data = pd.read_csv('./train.csv')\n",
    "    data.drop(['id'], axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)    \n",
    "    return data\n",
    "\n",
    "def testInjest():\n",
    "    data_test = pd.read_csv('./test.csv')\n",
    "    data_test.drop(['id'], axis=1, inplace=True)\n",
    "    return data_test\n",
    "\n",
    "data = ingest()\n",
    "data_test = testInjest()\n",
    "data.head(5)\n",
    "data.shape\n",
    "data['comment_text'] = data['comment_text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['comment_text'] = data['comment_text'].apply(lambda x: x.lower())\n",
    "\n",
    "data_test['comment_text'] = data_test['comment_text'].apply((lambda x: re.sub('[^a-zA-z]',' ',str(x))))\n",
    "data_test['comment_text'] = data_test['comment_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orphaned non free media  image   cd jboevl  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kentuckiana is colloquial   even though the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello fellow wikipedians  i have just modified...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>akc suspensions  the morning call   feb       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[wiki_link  talk celts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hello xdes  [wiki_link  wikipedia introduction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hi there  i m   i just wanted to let you know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i don t think a stuffed arm really counts as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>was it ever really a single in the uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thanks for uploading [wiki_link  image cloudco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text\n",
       "0    orphaned non free media  image   cd jboevl  ...\n",
       "1    kentuckiana is colloquial   even though the ...\n",
       "2  hello fellow wikipedians  i have just modified...\n",
       "3  akc suspensions  the morning call   feb       ...\n",
       "4                         [wiki_link  talk celts]   \n",
       "5  hello xdes  [wiki_link  wikipedia introduction...\n",
       "6  hi there  i m   i just wanted to let you know ...\n",
       "7    i don t think a stuffed arm really counts as...\n",
       "8           was it ever really a single in the uk   \n",
       "9  thanks for uploading [wiki_link  image cloudco..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226998, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "#        tweet = unicode(tweet.decode('utf-8').lower())\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "#         tokens = filter(lambda t: not t.startswith('@'), tokens)\n",
    "#         tokens = filter(lambda t: not t.startswith('#'), tokens)\n",
    "#         tokens = filter(lambda t: not t.startswith('http'), tokens)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 90000/90000 [00:21<00:00, 4114.16it/s]\n",
      "progress-bar: 100%|██████████| 226998/226998 [01:07<00:00, 3368.75it/s]\n"
     ]
    }
   ],
   "source": [
    "def postprocess(data, n=90000):\n",
    "    data = data.head(n)\n",
    "    data['comment_text'] = data['comment_text'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "#    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "def postprocess1(data, n=226998):\n",
    "    data = data.head(n)\n",
    "    data['comment_text'] = data['comment_text'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "#    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "data = postprocess(data)\n",
    "data_test = postprocess1(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[nonsense, kiss, off, geek, what, i, said, is,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[please, do, not, vandalize, pages, as, you, d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[points, of, interest, i, removed, the, points...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[asking, some, his, nationality, is, a, racial...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, reader, here, is, not, going, by, my, sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  [nonsense, kiss, off, geek, what, i, said, is,...      1             0   \n",
       "1  [please, do, not, vandalize, pages, as, you, d...      0             0   \n",
       "2  [points, of, interest, i, removed, the, points...      0             0   \n",
       "3  [asking, some, his, nationality, is, a, racial...      0             0   \n",
       "4  [the, reader, here, is, not, going, by, my, sa...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \n",
       "0        0       0       0              0  \n",
       "1        0       0       0              0  \n",
       "2        0       0       0              0  \n",
       "3        0       0       0              0  \n",
       "4        0       0       0              0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226998, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=90000\n",
    "x = np.array(data.head(n).comment_text)\n",
    "xpTest = np.array(data_test.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ list(['nonsense', 'kiss', 'off', 'geek', 'what', 'i', 'said', 'is', 'true', 'ill', 'have', 'your', 'account', 'terminated']),\n",
       "       list(['please', 'do', 'not', 'vandalize', 'pages', 'as', 'you', 'did', 'with', 'this', 'edit', 'to', 'w', 's', 'merwin', 'if', 'you', 'continue', 'to', 'do', 'so', 'you', 'will', 'be', 'blocked', 'from', 'editing']),\n",
       "       list(['points', 'of', 'interest', 'i', 'removed', 'the', 'points', 'of', 'interest', 'section', 'you', 'added', 'because', 'it', 'seemed', 'kind', 'of', 'spammy', 'i', 'know', 'you', 'probably', 'didnt', 'mean', 'to', 'disobey', 'the', 'rules', 'but', 'generally', 'a', 'point', 'of', 'interest', 'tends', 'to', 'be', 'rather', 'touristy', 'and', 'quite', 'irrelevant', 'to', 'an', 'area', 'culture', 'thats', 'just', 'my', 'opinion', 'though', 'if', 'you', 'want', 'to', 'reply', 'just', 'put', 'your', 'reply', 'here', 'and', 'add', 'talkbackjamiegraham', '08', 'on', 'my', 'talkpage']),\n",
       "       ...,\n",
       "       list(['history', 'of', 'the', 'term', 'vlah', 'according', 'to', 'the', 'wikipedia', 'the', 'term', 'was', 'derogatory', 'racist', 'so', 'i', 'want', 'some', 'proof', 'anything', 'at', 'all', 'really', 'that', 'states', 'that', 'this', 'was', 'an', 'umbrella', 'term', 'i', 'have', 'searched', 'google', 'for', 'vlah', 'serb', 'umbrella', 'term', 'and', 'got', 'no', 'results', 'im', 'asking', 'for', 'proof', 'because', 'like', 'i', 'said', 'its', 'a', 'racial', 'term', 'and', 'i', 'found', 'no', 'evidence', 'for', 'its', 'use']),\n",
       "       list(['redirect', 'talklist', 'of', 'agrawal', 'people']),\n",
       "       list(['then', 'i', 'guess', 'well', 'peel', 'this', 'onion', 'again', 'i', 'always', 'enjoy', 'a', 'good', 'spanking', 'this', 'morning', 'i', 'shot', 'an', 'elephant', 'in', 'my', 'underpants', 'how', 'he', 'got', 'in', 'there', 'ill', 'never', 'know'])], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226998"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xpTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data.iloc[ : , [1,2,3,4,5,6]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                    y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72000it [00:00, 252579.52it/s]\n",
      "18000it [00:00, 229859.86it/s]\n",
      "226998it [00:02, 83107.69it/s] \n"
     ]
    }
   ],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')\n",
    "xpTest = labelizeTweets(xpTest, 'xpTEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226998"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xpTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72000/72000 [00:00<00:00, 1306088.60it/s]\n",
      "100%|██████████| 72000/72000 [00:00<00:00, 1664287.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34116603"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dim = 200\n",
    "tweet_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(x_train)],total_examples=10000, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31690887,  0.09564571, -0.43064636, -0.19023147, -0.19107437,\n",
       "       -0.18558665,  0.65920138, -0.35802716, -0.02900002, -0.82409728,\n",
       "       -0.74998695, -0.69745839,  0.70544082,  1.46459186, -0.00538704,\n",
       "       -1.19610989,  0.44915837,  0.61099321, -0.27231115, -0.70463902,\n",
       "       -0.5501985 , -0.03105429, -0.01823857, -0.77969551, -0.29123259,\n",
       "       -0.76161528, -0.5721913 ,  0.44861951, -0.89943337, -1.25175822,\n",
       "       -0.11477406,  0.19114852, -0.1166639 ,  0.33315185,  0.18687882,\n",
       "        1.95786095, -0.50527388,  0.06508212, -1.41979373,  1.05057168,\n",
       "        0.28984976,  0.78972727,  0.25704905,  0.82874626,  1.01396799,\n",
       "        0.79089791,  0.16578628, -0.74659199, -0.91490722, -0.50939864,\n",
       "        0.23889644,  0.65001953, -0.25641647, -0.26716796,  0.57625043,\n",
       "       -1.43881524, -0.96519917,  0.02352213,  0.53389335, -0.16220026,\n",
       "        0.34284717,  0.87444288,  0.57828873, -1.07235849, -0.30634731,\n",
       "       -0.29580462,  0.05337659, -0.61276406, -0.3762905 , -1.06545484,\n",
       "        0.23780243,  1.07522321, -0.64096177,  0.2277261 ,  0.55480611,\n",
       "       -0.46923515,  0.42677248,  2.06725192, -0.20769414, -0.57444191,\n",
       "       -0.6037854 , -1.01215792,  0.41496727,  0.10793308,  0.2555266 ,\n",
       "        0.43246305, -0.05401613,  0.11283614, -0.20784582, -0.1263276 ,\n",
       "       -0.51229072,  0.12082882, -0.471158  , -0.02871228,  0.07128819,\n",
       "       -0.32929912, -0.34323087,  0.55149615, -0.54858351, -1.45533967,\n",
       "        0.30352008, -0.39762276, -0.2929965 ,  1.4566071 ,  0.18945755,\n",
       "        0.76958227, -1.1148268 ,  0.42271256,  0.25914428, -0.27712998,\n",
       "       -0.11656203, -0.71298683, -0.54276013, -1.26034975, -1.1215893 ,\n",
       "       -0.52895367,  0.57493663,  0.67058772,  0.12984794, -0.38854614,\n",
       "        0.26207101,  0.84143323,  0.2414974 ,  0.53790647,  1.2605052 ,\n",
       "        0.2994051 , -0.4463664 ,  0.08872381,  0.53722245, -1.46373975,\n",
       "       -0.14374185,  2.20976567, -0.00950673, -0.19611095, -0.45113429,\n",
       "       -0.98747212,  0.31940532,  0.76005322,  0.06184098,  0.0159074 ,\n",
       "        0.26814407,  0.96962565, -0.13143918, -0.14136644, -0.18340993,\n",
       "        0.12964787, -0.16012147, -0.25119066, -1.28622913,  0.40984321,\n",
       "        0.70076746, -0.41386512, -0.3238402 , -0.98387533,  0.86855865,\n",
       "        0.01866223,  0.64248389, -1.28212416,  0.86728573,  0.59254861,\n",
       "       -0.26960623,  0.68019485,  0.87195045, -1.47480869,  0.07903083,\n",
       "       -0.14678404, -1.03696752, -0.63967901, -0.08177831,  0.21407199,\n",
       "        0.89659393, -0.41780469,  0.6105594 , -0.5714646 ,  0.05045202,\n",
       "        0.77316999,  0.65056229,  0.16414845, -0.5413264 , -1.13233316,\n",
       "        0.29236978,  0.26494092,  1.37082744,  0.16338256,  0.58304733,\n",
       "        0.28353891, -0.55806547, -0.39166197,  0.99003375, -1.30645299,\n",
       "        0.75087249,  1.83350873,  0.25306192, -0.59751457, -0.91798985,\n",
       "        0.02746653,  0.01802888, -0.26392758, -0.51142013, -0.4481504 ], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', 0.7510071992874146),\n",
       " ('nice', 0.7141897678375244),\n",
       " ('taking', 0.6639406085014343),\n",
       " ('great', 0.6470977067947388),\n",
       " ('hard', 0.6450170278549194),\n",
       " ('serious', 0.6316549181938171),\n",
       " ('reasonable', 0.6301519274711609),\n",
       " ('wiki', 0.6293802261352539),\n",
       " ('little', 0.6019768714904785),\n",
       " ('fine', 0.5994066596031189)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing bokeh library for interactive dataviz\n",
    "# import bokeh.plotting as bp\n",
    "# from bokeh.models import HoverTool, BoxSelectTool\n",
    "# from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the chart\n",
    "# output_notebook()\n",
    "# plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"A map of 10000 word vectors\",\n",
    "#     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "#     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "# # getting a list of word vectors. limit to 10000. each is of 200 dimensions\n",
    "# word_vectors = [tweet_w2v[w] for w in list(tweet_w2v.wv.vocab.keys())[:5000]]\n",
    "\n",
    "# # dimensionality reduction. converting the vectors to 2d vectors\n",
    "# from sklearn.manifold import TSNE\n",
    "# tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "# tsne_w2v = tsne_model.fit_transform(word_vectors)\n",
    "\n",
    "# # putting everything in a dataframe\n",
    "# tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "# tsne_df['words'] = list(tweet_w2v.wv.vocab.keys())[:5000]\n",
    "\n",
    "# # plotting. the corresponding word appears when you hover on the data point.\n",
    "# plot_tfidf.scatter(x='x', y='y', source=tsne_df)\n",
    "# hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "# hover.tooltips={\"word\": \"@words\"}\n",
    "# show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 14134\n"
     ]
    }
   ],
   "source": [
    "print ('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72000it [00:40, 1763.67it/s]\n",
      "18000it [00:11, 1600.68it/s]\n",
      "226998it [02:23, 1587.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "Finaltest_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, xpTest))])\n",
    "Finaltest_vecs_w2v = scale(Finaltest_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72000/72000 [==============================] - 9s 124us/step - loss: 0.3029 - acc: 0.9610\n",
      "Epoch 2/100\n",
      "72000/72000 [==============================] - 6s 85us/step - loss: 0.2967 - acc: 0.9900\n",
      "Epoch 3/100\n",
      "72000/72000 [==============================] - 6s 85us/step - loss: 0.2946 - acc: 0.9903\n",
      "Epoch 4/100\n",
      "72000/72000 [==============================] - 6s 84us/step - loss: 0.2936 - acc: 0.9904\n",
      "Epoch 5/100\n",
      "72000/72000 [==============================] - 6s 85us/step - loss: 0.2923 - acc: 0.9904\n",
      "Epoch 6/100\n",
      "72000/72000 [==============================] - 6s 88us/step - loss: 0.2920 - acc: 0.9903\n",
      "Epoch 7/100\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.2916 - acc: 0.9898\n",
      "Epoch 8/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2910 - acc: 0.9901\n",
      "Epoch 9/100\n",
      "72000/72000 [==============================] - 6s 88us/step - loss: 0.2905 - acc: 0.9891\n",
      "Epoch 10/100\n",
      "72000/72000 [==============================] - 6s 88us/step - loss: 0.2908 - acc: 0.9879\n",
      "Epoch 11/100\n",
      "72000/72000 [==============================] - 6s 87us/step - loss: 0.2900 - acc: 0.9895\n",
      "Epoch 12/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2899 - acc: 0.9891\n",
      "Epoch 13/100\n",
      "72000/72000 [==============================] - 6s 87us/step - loss: 0.2899 - acc: 0.9892\n",
      "Epoch 14/100\n",
      "72000/72000 [==============================] - 6s 88us/step - loss: 0.2895 - acc: 0.9894\n",
      "Epoch 15/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2891 - acc: 0.9883\n",
      "Epoch 16/100\n",
      "72000/72000 [==============================] - 6s 90us/step - loss: 0.2897 - acc: 0.9890\n",
      "Epoch 17/100\n",
      "72000/72000 [==============================] - 7s 98us/step - loss: 0.2896 - acc: 0.9877\n",
      "Epoch 18/100\n",
      "72000/72000 [==============================] - 7s 96us/step - loss: 0.2890 - acc: 0.9877\n",
      "Epoch 19/100\n",
      "72000/72000 [==============================] - 6s 90us/step - loss: 0.2887 - acc: 0.9881\n",
      "Epoch 20/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2889 - acc: 0.9876\n",
      "Epoch 21/100\n",
      "72000/72000 [==============================] - 6s 87us/step - loss: 0.2887 - acc: 0.9873\n",
      "Epoch 22/100\n",
      "72000/72000 [==============================] - 6s 87us/step - loss: 0.2889 - acc: 0.9893\n",
      "Epoch 23/100\n",
      "72000/72000 [==============================] - 6s 88us/step - loss: 0.2886 - acc: 0.9895\n",
      "Epoch 24/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2885 - acc: 0.9869\n",
      "Epoch 25/100\n",
      "72000/72000 [==============================] - 6s 88us/step - loss: 0.2880 - acc: 0.9882\n",
      "Epoch 26/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2886 - acc: 0.9889\n",
      "Epoch 27/100\n",
      "72000/72000 [==============================] - 6s 90us/step - loss: 0.2878 - acc: 0.9887\n",
      "Epoch 28/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2882 - acc: 0.9876\n",
      "Epoch 29/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2876 - acc: 0.9880\n",
      "Epoch 30/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2877 - acc: 0.9875\n",
      "Epoch 31/100\n",
      "72000/72000 [==============================] - 6s 90us/step - loss: 0.2873 - acc: 0.9867\n",
      "Epoch 32/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2878 - acc: 0.9867\n",
      "Epoch 33/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2874 - acc: 0.9883\n",
      "Epoch 34/100\n",
      "72000/72000 [==============================] - 6s 90us/step - loss: 0.2875 - acc: 0.9872\n",
      "Epoch 35/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2877 - acc: 0.9858\n",
      "Epoch 36/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2873 - acc: 0.9854\n",
      "Epoch 37/100\n",
      "72000/72000 [==============================] - 7s 91us/step - loss: 0.2871 - acc: 0.9845\n",
      "Epoch 38/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2868 - acc: 0.9868\n",
      "Epoch 39/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2871 - acc: 0.9838\n",
      "Epoch 40/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2872 - acc: 0.9873\n",
      "Epoch 41/100\n",
      "72000/72000 [==============================] - 7s 91us/step - loss: 0.2875 - acc: 0.9855\n",
      "Epoch 42/100\n",
      "72000/72000 [==============================] - 6s 90us/step - loss: 0.2874 - acc: 0.9873\n",
      "Epoch 43/100\n",
      "72000/72000 [==============================] - 6s 90us/step - loss: 0.2876 - acc: 0.9856\n",
      "Epoch 44/100\n",
      "72000/72000 [==============================] - 6s 90us/step - loss: 0.2867 - acc: 0.9869\n",
      "Epoch 45/100\n",
      "72000/72000 [==============================] - 7s 95us/step - loss: 0.2866 - acc: 0.9860\n",
      "Epoch 46/100\n",
      "72000/72000 [==============================] - 7s 97us/step - loss: 0.2876 - acc: 0.9859\n",
      "Epoch 47/100\n",
      "72000/72000 [==============================] - 7s 95us/step - loss: 0.2866 - acc: 0.9832\n",
      "Epoch 48/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2869 - acc: 0.9865\n",
      "Epoch 49/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2867 - acc: 0.9848\n",
      "Epoch 50/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2871 - acc: 0.9830\n",
      "Epoch 51/100\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.2867 - acc: 0.9841\n",
      "Epoch 52/100\n",
      "72000/72000 [==============================] - 7s 90us/step - loss: 0.2864 - acc: 0.9852\n",
      "Epoch 53/100\n",
      "72000/72000 [==============================] - 6s 90us/step - loss: 0.2868 - acc: 0.9838\n",
      "Epoch 54/100\n",
      "72000/72000 [==============================] - 8s 104us/step - loss: 0.2862 - acc: 0.9834\n",
      "Epoch 55/100\n",
      "72000/72000 [==============================] - 7s 101us/step - loss: 0.2864 - acc: 0.9852\n",
      "Epoch 56/100\n",
      "72000/72000 [==============================] - 7s 95us/step - loss: 0.2864 - acc: 0.9840\n",
      "Epoch 57/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2870 - acc: 0.9867\n",
      "Epoch 58/100\n",
      "72000/72000 [==============================] - 7s 94us/step - loss: 0.2871 - acc: 0.9874\n",
      "Epoch 59/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2865 - acc: 0.9868\n",
      "Epoch 60/100\n",
      "72000/72000 [==============================] - 7s 93us/step - loss: 0.2865 - acc: 0.9870\n",
      "Epoch 61/100\n",
      "72000/72000 [==============================] - 7s 93us/step - loss: 0.2859 - acc: 0.9858\n",
      "Epoch 62/100\n",
      "72000/72000 [==============================] - 7s 94us/step - loss: 0.2867 - acc: 0.9866\n",
      "Epoch 63/100\n",
      "72000/72000 [==============================] - 7s 94us/step - loss: 0.2865 - acc: 0.9859\n",
      "Epoch 64/100\n",
      "72000/72000 [==============================] - 7s 95us/step - loss: 0.2866 - acc: 0.9844\n",
      "Epoch 65/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2860 - acc: 0.9839\n",
      "Epoch 66/100\n",
      "72000/72000 [==============================] - 7s 95us/step - loss: 0.2877 - acc: 0.9844\n",
      "Epoch 67/100\n",
      "72000/72000 [==============================] - 7s 95us/step - loss: 0.2862 - acc: 0.9831\n",
      "Epoch 68/100\n",
      "72000/72000 [==============================] - 7s 94us/step - loss: 0.2862 - acc: 0.9816\n",
      "Epoch 69/100\n",
      "72000/72000 [==============================] - 6s 90us/step - loss: 0.2862 - acc: 0.9813\n",
      "Epoch 70/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2864 - acc: 0.9846\n",
      "Epoch 71/100\n",
      "72000/72000 [==============================] - 7s 93us/step - loss: 0.2865 - acc: 0.9829\n",
      "Epoch 72/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2865 - acc: 0.9842\n",
      "Epoch 73/100\n",
      "72000/72000 [==============================] - 7s 99us/step - loss: 0.2862 - acc: 0.9823\n",
      "Epoch 74/100\n",
      "72000/72000 [==============================] - 7s 94us/step - loss: 0.2852 - acc: 0.9853\n",
      "Epoch 75/100\n",
      "72000/72000 [==============================] - 7s 93us/step - loss: 0.2856 - acc: 0.9832\n",
      "Epoch 76/100\n",
      "72000/72000 [==============================] - 7s 94us/step - loss: 0.2855 - acc: 0.9849\n",
      "Epoch 77/100\n",
      "72000/72000 [==============================] - 7s 95us/step - loss: 0.2865 - acc: 0.9821\n",
      "Epoch 78/100\n",
      "72000/72000 [==============================] - 7s 94us/step - loss: 0.2857 - acc: 0.9835\n",
      "Epoch 79/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2866 - acc: 0.9828\n",
      "Epoch 80/100\n",
      "72000/72000 [==============================] - 7s 94us/step - loss: 0.2868 - acc: 0.9824\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000/72000 [==============================] - 6s 77us/step - loss: 0.2856 - acc: 0.9817\n",
      "Epoch 82/100\n",
      "72000/72000 [==============================] - 6s 77us/step - loss: 0.2855 - acc: 0.9807\n",
      "Epoch 83/100\n",
      "72000/72000 [==============================] - 6s 77us/step - loss: 0.2858 - acc: 0.9798\n",
      "Epoch 84/100\n",
      "72000/72000 [==============================] - 6s 77us/step - loss: 0.2851 - acc: 0.9825\n",
      "Epoch 85/100\n",
      "72000/72000 [==============================] - 6s 86us/step - loss: 0.2850 - acc: 0.9811\n",
      "Epoch 86/100\n",
      "72000/72000 [==============================] - 6s 87us/step - loss: 0.2864 - acc: 0.9843\n",
      "Epoch 87/100\n",
      "72000/72000 [==============================] - 6s 88us/step - loss: 0.2855 - acc: 0.9816\n",
      "Epoch 88/100\n",
      "72000/72000 [==============================] - 6s 87us/step - loss: 0.2853 - acc: 0.9831\n",
      "Epoch 89/100\n",
      "72000/72000 [==============================] - 6s 88us/step - loss: 0.2853 - acc: 0.9860\n",
      "Epoch 90/100\n",
      "72000/72000 [==============================] - 6s 88us/step - loss: 0.2856 - acc: 0.9841\n",
      "Epoch 91/100\n",
      "72000/72000 [==============================] - 6s 88us/step - loss: 0.2855 - acc: 0.9858\n",
      "Epoch 92/100\n",
      "72000/72000 [==============================] - 7s 96us/step - loss: 0.2858 - acc: 0.9816\n",
      "Epoch 93/100\n",
      "72000/72000 [==============================] - 7s 93us/step - loss: 0.2868 - acc: 0.9809\n",
      "Epoch 94/100\n",
      "72000/72000 [==============================] - 7s 95us/step - loss: 0.2855 - acc: 0.9808\n",
      "Epoch 95/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2850 - acc: 0.9805\n",
      "Epoch 96/100\n",
      "72000/72000 [==============================] - 7s 101us/step - loss: 0.2856 - acc: 0.9765\n",
      "Epoch 97/100\n",
      "72000/72000 [==============================] - 7s 102us/step - loss: 0.2855 - acc: 0.9837\n",
      "Epoch 98/100\n",
      "72000/72000 [==============================] - 7s 97us/step - loss: 0.2853 - acc: 0.9831\n",
      "Epoch 99/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2857 - acc: 0.9816\n",
      "Epoch 100/100\n",
      "72000/72000 [==============================] - 7s 92us/step - loss: 0.2856 - acc: 0.9839\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(103, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(103, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(103, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=32)\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99066666672\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
    "print (score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29633516761991713, 0.99066666671964854]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(Finaltest_vecs_w2v, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.54658538e-01,   2.98151504e-02,   2.04153940e-01,\n",
       "          5.64452484e-02,   2.10177258e-01,   4.47498634e-02],\n",
       "       [  4.99796838e-01,   1.71534140e-02,   2.03638375e-01,\n",
       "          1.33441174e-02,   2.19956189e-01,   4.61110063e-02],\n",
       "       [  5.07121027e-01,   1.35288248e-02,   2.36039609e-01,\n",
       "          3.15914187e-03,   2.26269841e-01,   1.38815334e-02],\n",
       "       ..., \n",
       "       [  4.30456221e-01,   3.52740176e-02,   2.15408146e-01,\n",
       "          4.99149039e-02,   2.31405586e-01,   3.75411101e-02],\n",
       "       [  6.28165424e-01,   1.01553532e-03,   1.39932096e-01,\n",
       "          1.42187173e-06,   1.88268512e-01,   4.26170714e-02],\n",
       "       [  5.21906197e-01,   7.80870626e-03,   2.18505949e-01,\n",
       "          1.67631975e-03,   2.33591676e-01,   1.65112149e-02]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[: , 0]\n",
    "px = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'toxic':px[:,0],'severe_toxic':px[:,1],'obscene':px[:,2] , 'threat':px[:,3] , 'insult':px[:,4] ,'identity_hate':px[:,5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044750</td>\n",
       "      <td>0.210177</td>\n",
       "      <td>0.204154</td>\n",
       "      <td>2.981515e-02</td>\n",
       "      <td>5.644525e-02</td>\n",
       "      <td>0.454659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.046111</td>\n",
       "      <td>0.219956</td>\n",
       "      <td>0.203638</td>\n",
       "      <td>1.715341e-02</td>\n",
       "      <td>1.334412e-02</td>\n",
       "      <td>0.499797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.226270</td>\n",
       "      <td>0.236040</td>\n",
       "      <td>1.352882e-02</td>\n",
       "      <td>3.159142e-03</td>\n",
       "      <td>0.507121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003143</td>\n",
       "      <td>0.193943</td>\n",
       "      <td>0.188158</td>\n",
       "      <td>1.055632e-03</td>\n",
       "      <td>5.926859e-05</td>\n",
       "      <td>0.613642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010243</td>\n",
       "      <td>0.214843</td>\n",
       "      <td>0.208896</td>\n",
       "      <td>4.666849e-03</td>\n",
       "      <td>4.894359e-03</td>\n",
       "      <td>0.556457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.034504</td>\n",
       "      <td>0.214555</td>\n",
       "      <td>0.205472</td>\n",
       "      <td>1.977843e-02</td>\n",
       "      <td>2.516032e-02</td>\n",
       "      <td>0.500531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.013886</td>\n",
       "      <td>0.223984</td>\n",
       "      <td>0.226706</td>\n",
       "      <td>8.801127e-03</td>\n",
       "      <td>1.677737e-03</td>\n",
       "      <td>0.524944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.022846</td>\n",
       "      <td>0.227198</td>\n",
       "      <td>0.229686</td>\n",
       "      <td>2.135832e-02</td>\n",
       "      <td>9.110350e-03</td>\n",
       "      <td>0.489801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.024413</td>\n",
       "      <td>0.215681</td>\n",
       "      <td>0.212810</td>\n",
       "      <td>8.235474e-03</td>\n",
       "      <td>2.251629e-03</td>\n",
       "      <td>0.536609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.009026</td>\n",
       "      <td>0.215330</td>\n",
       "      <td>0.237528</td>\n",
       "      <td>1.766505e-02</td>\n",
       "      <td>5.619880e-03</td>\n",
       "      <td>0.514831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.021120</td>\n",
       "      <td>0.215644</td>\n",
       "      <td>0.218386</td>\n",
       "      <td>2.620252e-02</td>\n",
       "      <td>2.561421e-02</td>\n",
       "      <td>0.493034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.045055</td>\n",
       "      <td>0.222560</td>\n",
       "      <td>0.201250</td>\n",
       "      <td>1.628778e-02</td>\n",
       "      <td>1.305437e-02</td>\n",
       "      <td>0.501792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.035862</td>\n",
       "      <td>0.195065</td>\n",
       "      <td>0.147488</td>\n",
       "      <td>7.089384e-04</td>\n",
       "      <td>4.372406e-05</td>\n",
       "      <td>0.620833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.080179</td>\n",
       "      <td>0.196796</td>\n",
       "      <td>0.158293</td>\n",
       "      <td>4.936939e-03</td>\n",
       "      <td>2.019779e-02</td>\n",
       "      <td>0.539598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.209235</td>\n",
       "      <td>0.192240</td>\n",
       "      <td>1.063328e-03</td>\n",
       "      <td>3.895842e-05</td>\n",
       "      <td>0.592463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.038268</td>\n",
       "      <td>0.220235</td>\n",
       "      <td>0.204743</td>\n",
       "      <td>1.558773e-02</td>\n",
       "      <td>1.509309e-02</td>\n",
       "      <td>0.506074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005707</td>\n",
       "      <td>0.200005</td>\n",
       "      <td>0.200236</td>\n",
       "      <td>1.149165e-03</td>\n",
       "      <td>4.566925e-05</td>\n",
       "      <td>0.592858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.066548</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>1.996986e-09</td>\n",
       "      <td>7.468835e-15</td>\n",
       "      <td>0.861193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.016402</td>\n",
       "      <td>0.181985</td>\n",
       "      <td>0.165718</td>\n",
       "      <td>6.492023e-04</td>\n",
       "      <td>1.905710e-05</td>\n",
       "      <td>0.635228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.043729</td>\n",
       "      <td>0.221875</td>\n",
       "      <td>0.193972</td>\n",
       "      <td>1.047506e-02</td>\n",
       "      <td>5.915682e-03</td>\n",
       "      <td>0.524034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.031358</td>\n",
       "      <td>0.220927</td>\n",
       "      <td>0.211824</td>\n",
       "      <td>1.469135e-02</td>\n",
       "      <td>8.651530e-03</td>\n",
       "      <td>0.512548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005663</td>\n",
       "      <td>0.208352</td>\n",
       "      <td>0.224724</td>\n",
       "      <td>3.443298e-03</td>\n",
       "      <td>1.967025e-04</td>\n",
       "      <td>0.557621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.208855</td>\n",
       "      <td>0.245209</td>\n",
       "      <td>3.630126e-03</td>\n",
       "      <td>8.618018e-05</td>\n",
       "      <td>0.538450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.009190</td>\n",
       "      <td>0.216369</td>\n",
       "      <td>0.237406</td>\n",
       "      <td>1.732125e-02</td>\n",
       "      <td>5.120834e-03</td>\n",
       "      <td>0.514592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.039573</td>\n",
       "      <td>0.221328</td>\n",
       "      <td>0.202988</td>\n",
       "      <td>1.538113e-02</td>\n",
       "      <td>1.272157e-02</td>\n",
       "      <td>0.508008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.020789</td>\n",
       "      <td>0.207201</td>\n",
       "      <td>0.220371</td>\n",
       "      <td>3.520375e-02</td>\n",
       "      <td>5.648041e-02</td>\n",
       "      <td>0.459955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.213297</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>4.299011e-03</td>\n",
       "      <td>1.274674e-03</td>\n",
       "      <td>0.569374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.011003</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>0.134380</td>\n",
       "      <td>1.155762e-02</td>\n",
       "      <td>2.717809e-01</td>\n",
       "      <td>0.448072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.222515</td>\n",
       "      <td>0.227248</td>\n",
       "      <td>2.265146e-03</td>\n",
       "      <td>1.538884e-04</td>\n",
       "      <td>0.544696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.089869</td>\n",
       "      <td>0.096985</td>\n",
       "      <td>6.862435e-08</td>\n",
       "      <td>1.023301e-12</td>\n",
       "      <td>0.813127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226968</th>\n",
       "      <td>0.036706</td>\n",
       "      <td>0.208590</td>\n",
       "      <td>0.168162</td>\n",
       "      <td>2.479545e-03</td>\n",
       "      <td>3.666762e-04</td>\n",
       "      <td>0.583696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226969</th>\n",
       "      <td>0.015468</td>\n",
       "      <td>0.210298</td>\n",
       "      <td>0.206446</td>\n",
       "      <td>2.423349e-02</td>\n",
       "      <td>5.352122e-02</td>\n",
       "      <td>0.490033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226970</th>\n",
       "      <td>0.135625</td>\n",
       "      <td>0.178252</td>\n",
       "      <td>0.123954</td>\n",
       "      <td>1.497561e-03</td>\n",
       "      <td>5.030314e-03</td>\n",
       "      <td>0.555641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226971</th>\n",
       "      <td>0.040851</td>\n",
       "      <td>0.220610</td>\n",
       "      <td>0.202560</td>\n",
       "      <td>1.541574e-02</td>\n",
       "      <td>1.393563e-02</td>\n",
       "      <td>0.506627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226972</th>\n",
       "      <td>0.026648</td>\n",
       "      <td>0.206270</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>2.983670e-02</td>\n",
       "      <td>6.813759e-02</td>\n",
       "      <td>0.466772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226973</th>\n",
       "      <td>0.026269</td>\n",
       "      <td>0.223792</td>\n",
       "      <td>0.213373</td>\n",
       "      <td>1.236905e-02</td>\n",
       "      <td>9.668367e-03</td>\n",
       "      <td>0.514529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226974</th>\n",
       "      <td>0.109917</td>\n",
       "      <td>0.199535</td>\n",
       "      <td>0.128947</td>\n",
       "      <td>1.278153e-03</td>\n",
       "      <td>6.743849e-05</td>\n",
       "      <td>0.560255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226975</th>\n",
       "      <td>0.020555</td>\n",
       "      <td>0.227127</td>\n",
       "      <td>0.210062</td>\n",
       "      <td>8.883119e-03</td>\n",
       "      <td>8.568224e-03</td>\n",
       "      <td>0.524805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226976</th>\n",
       "      <td>0.034523</td>\n",
       "      <td>0.232118</td>\n",
       "      <td>0.229673</td>\n",
       "      <td>2.770785e-02</td>\n",
       "      <td>1.515831e-02</td>\n",
       "      <td>0.460820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226977</th>\n",
       "      <td>0.012806</td>\n",
       "      <td>0.219796</td>\n",
       "      <td>0.203203</td>\n",
       "      <td>3.991871e-03</td>\n",
       "      <td>1.923872e-03</td>\n",
       "      <td>0.558280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226978</th>\n",
       "      <td>0.035540</td>\n",
       "      <td>0.214785</td>\n",
       "      <td>0.199851</td>\n",
       "      <td>8.351101e-03</td>\n",
       "      <td>3.396101e-03</td>\n",
       "      <td>0.538077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226979</th>\n",
       "      <td>0.003915</td>\n",
       "      <td>0.203368</td>\n",
       "      <td>0.217555</td>\n",
       "      <td>1.188838e-03</td>\n",
       "      <td>1.124376e-05</td>\n",
       "      <td>0.573962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226980</th>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.215383</td>\n",
       "      <td>0.228624</td>\n",
       "      <td>4.496065e-03</td>\n",
       "      <td>2.048999e-04</td>\n",
       "      <td>0.546291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226981</th>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.221868</td>\n",
       "      <td>0.239003</td>\n",
       "      <td>4.298931e-03</td>\n",
       "      <td>8.874028e-04</td>\n",
       "      <td>0.529476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226982</th>\n",
       "      <td>0.002735</td>\n",
       "      <td>0.204123</td>\n",
       "      <td>0.276072</td>\n",
       "      <td>5.186465e-03</td>\n",
       "      <td>2.076182e-05</td>\n",
       "      <td>0.511863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226983</th>\n",
       "      <td>0.011517</td>\n",
       "      <td>0.214117</td>\n",
       "      <td>0.227682</td>\n",
       "      <td>2.219570e-02</td>\n",
       "      <td>1.414359e-02</td>\n",
       "      <td>0.510345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226984</th>\n",
       "      <td>0.005879</td>\n",
       "      <td>0.193792</td>\n",
       "      <td>0.199148</td>\n",
       "      <td>9.403396e-04</td>\n",
       "      <td>9.934945e-05</td>\n",
       "      <td>0.600142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226985</th>\n",
       "      <td>0.025613</td>\n",
       "      <td>0.194870</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>3.280167e-02</td>\n",
       "      <td>8.589018e-02</td>\n",
       "      <td>0.461775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226986</th>\n",
       "      <td>0.018209</td>\n",
       "      <td>0.231720</td>\n",
       "      <td>0.262259</td>\n",
       "      <td>2.430000e-02</td>\n",
       "      <td>2.871652e-03</td>\n",
       "      <td>0.460641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226987</th>\n",
       "      <td>0.012611</td>\n",
       "      <td>0.208858</td>\n",
       "      <td>0.228639</td>\n",
       "      <td>2.603657e-02</td>\n",
       "      <td>2.329303e-02</td>\n",
       "      <td>0.500563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226988</th>\n",
       "      <td>0.057168</td>\n",
       "      <td>0.207945</td>\n",
       "      <td>0.199185</td>\n",
       "      <td>1.277189e-02</td>\n",
       "      <td>1.759653e-02</td>\n",
       "      <td>0.505333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226989</th>\n",
       "      <td>0.009596</td>\n",
       "      <td>0.197529</td>\n",
       "      <td>0.218135</td>\n",
       "      <td>2.644353e-03</td>\n",
       "      <td>2.078866e-04</td>\n",
       "      <td>0.571888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226990</th>\n",
       "      <td>0.007547</td>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.232404</td>\n",
       "      <td>5.732422e-03</td>\n",
       "      <td>5.472365e-04</td>\n",
       "      <td>0.538017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226991</th>\n",
       "      <td>0.019245</td>\n",
       "      <td>0.181177</td>\n",
       "      <td>0.159950</td>\n",
       "      <td>5.760646e-04</td>\n",
       "      <td>2.765970e-05</td>\n",
       "      <td>0.639024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226992</th>\n",
       "      <td>0.012574</td>\n",
       "      <td>0.230563</td>\n",
       "      <td>0.225552</td>\n",
       "      <td>8.204734e-03</td>\n",
       "      <td>1.769024e-03</td>\n",
       "      <td>0.521337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226993</th>\n",
       "      <td>0.024277</td>\n",
       "      <td>0.224879</td>\n",
       "      <td>0.201927</td>\n",
       "      <td>2.478446e-02</td>\n",
       "      <td>3.785671e-02</td>\n",
       "      <td>0.486275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226994</th>\n",
       "      <td>0.047269</td>\n",
       "      <td>0.225272</td>\n",
       "      <td>0.202211</td>\n",
       "      <td>1.805049e-02</td>\n",
       "      <td>1.592846e-02</td>\n",
       "      <td>0.491268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226995</th>\n",
       "      <td>0.037541</td>\n",
       "      <td>0.231406</td>\n",
       "      <td>0.215408</td>\n",
       "      <td>3.527402e-02</td>\n",
       "      <td>4.991490e-02</td>\n",
       "      <td>0.430456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226996</th>\n",
       "      <td>0.042617</td>\n",
       "      <td>0.188269</td>\n",
       "      <td>0.139932</td>\n",
       "      <td>1.015535e-03</td>\n",
       "      <td>1.421872e-06</td>\n",
       "      <td>0.628165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226997</th>\n",
       "      <td>0.016511</td>\n",
       "      <td>0.233592</td>\n",
       "      <td>0.218506</td>\n",
       "      <td>7.808706e-03</td>\n",
       "      <td>1.676320e-03</td>\n",
       "      <td>0.521906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226998 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        identity_hate    insult   obscene  severe_toxic        threat  \\\n",
       "0            0.044750  0.210177  0.204154  2.981515e-02  5.644525e-02   \n",
       "1            0.046111  0.219956  0.203638  1.715341e-02  1.334412e-02   \n",
       "2            0.013882  0.226270  0.236040  1.352882e-02  3.159142e-03   \n",
       "3            0.003143  0.193943  0.188158  1.055632e-03  5.926859e-05   \n",
       "4            0.010243  0.214843  0.208896  4.666849e-03  4.894359e-03   \n",
       "5            0.034504  0.214555  0.205472  1.977843e-02  2.516032e-02   \n",
       "6            0.013886  0.223984  0.226706  8.801127e-03  1.677737e-03   \n",
       "7            0.022846  0.227198  0.229686  2.135832e-02  9.110350e-03   \n",
       "8            0.024413  0.215681  0.212810  8.235474e-03  2.251629e-03   \n",
       "9            0.009026  0.215330  0.237528  1.766505e-02  5.619880e-03   \n",
       "10           0.021120  0.215644  0.218386  2.620252e-02  2.561421e-02   \n",
       "11           0.045055  0.222560  0.201250  1.628778e-02  1.305437e-02   \n",
       "12           0.035862  0.195065  0.147488  7.089384e-04  4.372406e-05   \n",
       "13           0.080179  0.196796  0.158293  4.936939e-03  2.019779e-02   \n",
       "14           0.004960  0.209235  0.192240  1.063328e-03  3.895842e-05   \n",
       "15           0.038268  0.220235  0.204743  1.558773e-02  1.509309e-02   \n",
       "16           0.005707  0.200005  0.200236  1.149165e-03  4.566925e-05   \n",
       "17           0.000004  0.066548  0.072254  1.996986e-09  7.468835e-15   \n",
       "18           0.016402  0.181985  0.165718  6.492023e-04  1.905710e-05   \n",
       "19           0.043729  0.221875  0.193972  1.047506e-02  5.915682e-03   \n",
       "20           0.031358  0.220927  0.211824  1.469135e-02  8.651530e-03   \n",
       "21           0.005663  0.208352  0.224724  3.443298e-03  1.967025e-04   \n",
       "22           0.003770  0.208855  0.245209  3.630126e-03  8.618018e-05   \n",
       "23           0.009190  0.216369  0.237406  1.732125e-02  5.120834e-03   \n",
       "24           0.039573  0.221328  0.202988  1.538113e-02  1.272157e-02   \n",
       "25           0.020789  0.207201  0.220371  3.520375e-02  5.648041e-02   \n",
       "26           0.028255  0.213297  0.183500  4.299011e-03  1.274674e-03   \n",
       "27           0.011003  0.123207  0.134380  1.155762e-02  2.717809e-01   \n",
       "28           0.003122  0.222515  0.227248  2.265146e-03  1.538884e-04   \n",
       "29           0.000019  0.089869  0.096985  6.862435e-08  1.023301e-12   \n",
       "...               ...       ...       ...           ...           ...   \n",
       "226968       0.036706  0.208590  0.168162  2.479545e-03  3.666762e-04   \n",
       "226969       0.015468  0.210298  0.206446  2.423349e-02  5.352122e-02   \n",
       "226970       0.135625  0.178252  0.123954  1.497561e-03  5.030314e-03   \n",
       "226971       0.040851  0.220610  0.202560  1.541574e-02  1.393563e-02   \n",
       "226972       0.026648  0.206270  0.202336  2.983670e-02  6.813759e-02   \n",
       "226973       0.026269  0.223792  0.213373  1.236905e-02  9.668367e-03   \n",
       "226974       0.109917  0.199535  0.128947  1.278153e-03  6.743849e-05   \n",
       "226975       0.020555  0.227127  0.210062  8.883119e-03  8.568224e-03   \n",
       "226976       0.034523  0.232118  0.229673  2.770785e-02  1.515831e-02   \n",
       "226977       0.012806  0.219796  0.203203  3.991871e-03  1.923872e-03   \n",
       "226978       0.035540  0.214785  0.199851  8.351101e-03  3.396101e-03   \n",
       "226979       0.003915  0.203368  0.217555  1.188838e-03  1.124376e-05   \n",
       "226980       0.005001  0.215383  0.228624  4.496065e-03  2.048999e-04   \n",
       "226981       0.004466  0.221868  0.239003  4.298931e-03  8.874028e-04   \n",
       "226982       0.002735  0.204123  0.276072  5.186465e-03  2.076182e-05   \n",
       "226983       0.011517  0.214117  0.227682  2.219570e-02  1.414359e-02   \n",
       "226984       0.005879  0.193792  0.199148  9.403396e-04  9.934945e-05   \n",
       "226985       0.025613  0.194870  0.199051  3.280167e-02  8.589018e-02   \n",
       "226986       0.018209  0.231720  0.262259  2.430000e-02  2.871652e-03   \n",
       "226987       0.012611  0.208858  0.228639  2.603657e-02  2.329303e-02   \n",
       "226988       0.057168  0.207945  0.199185  1.277189e-02  1.759653e-02   \n",
       "226989       0.009596  0.197529  0.218135  2.644353e-03  2.078866e-04   \n",
       "226990       0.007547  0.215753  0.232404  5.732422e-03  5.472365e-04   \n",
       "226991       0.019245  0.181177  0.159950  5.760646e-04  2.765970e-05   \n",
       "226992       0.012574  0.230563  0.225552  8.204734e-03  1.769024e-03   \n",
       "226993       0.024277  0.224879  0.201927  2.478446e-02  3.785671e-02   \n",
       "226994       0.047269  0.225272  0.202211  1.805049e-02  1.592846e-02   \n",
       "226995       0.037541  0.231406  0.215408  3.527402e-02  4.991490e-02   \n",
       "226996       0.042617  0.188269  0.139932  1.015535e-03  1.421872e-06   \n",
       "226997       0.016511  0.233592  0.218506  7.808706e-03  1.676320e-03   \n",
       "\n",
       "           toxic  \n",
       "0       0.454659  \n",
       "1       0.499797  \n",
       "2       0.507121  \n",
       "3       0.613642  \n",
       "4       0.556457  \n",
       "5       0.500531  \n",
       "6       0.524944  \n",
       "7       0.489801  \n",
       "8       0.536609  \n",
       "9       0.514831  \n",
       "10      0.493034  \n",
       "11      0.501792  \n",
       "12      0.620833  \n",
       "13      0.539598  \n",
       "14      0.592463  \n",
       "15      0.506074  \n",
       "16      0.592858  \n",
       "17      0.861193  \n",
       "18      0.635228  \n",
       "19      0.524034  \n",
       "20      0.512548  \n",
       "21      0.557621  \n",
       "22      0.538450  \n",
       "23      0.514592  \n",
       "24      0.508008  \n",
       "25      0.459955  \n",
       "26      0.569374  \n",
       "27      0.448072  \n",
       "28      0.544696  \n",
       "29      0.813127  \n",
       "...          ...  \n",
       "226968  0.583696  \n",
       "226969  0.490033  \n",
       "226970  0.555641  \n",
       "226971  0.506627  \n",
       "226972  0.466772  \n",
       "226973  0.514529  \n",
       "226974  0.560255  \n",
       "226975  0.524805  \n",
       "226976  0.460820  \n",
       "226977  0.558280  \n",
       "226978  0.538077  \n",
       "226979  0.573962  \n",
       "226980  0.546291  \n",
       "226981  0.529476  \n",
       "226982  0.511863  \n",
       "226983  0.510345  \n",
       "226984  0.600142  \n",
       "226985  0.461775  \n",
       "226986  0.460641  \n",
       "226987  0.500563  \n",
       "226988  0.505333  \n",
       "226989  0.571888  \n",
       "226990  0.538017  \n",
       "226991  0.639024  \n",
       "226992  0.521337  \n",
       "226993  0.486275  \n",
       "226994  0.491268  \n",
       "226995  0.430456  \n",
       "226996  0.628165  \n",
       "226997  0.521906  \n",
       "\n",
       "[226998 rows x 6 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_csv = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = id_csv['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['identity_hate', 'insult', 'obscene', 'severe_toxic', 'threat', 'toxic', 'id']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[['id','toxic', 'severe_toxic','obscene', 'threat' , 'insult' ,'identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>0.454659</td>\n",
       "      <td>0.029815</td>\n",
       "      <td>0.204154</td>\n",
       "      <td>0.056445</td>\n",
       "      <td>0.210177</td>\n",
       "      <td>0.044750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>0.017153</td>\n",
       "      <td>0.203638</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>0.219956</td>\n",
       "      <td>0.046111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>0.507121</td>\n",
       "      <td>0.013529</td>\n",
       "      <td>0.236040</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>0.226270</td>\n",
       "      <td>0.013882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>0.613642</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.188158</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.193943</td>\n",
       "      <td>0.003143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>0.556457</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.208896</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.214843</td>\n",
       "      <td>0.010243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0   6044863  0.454659      0.029815  0.204154  0.056445  0.210177   \n",
       "1   6102620  0.499797      0.017153  0.203638  0.013344  0.219956   \n",
       "2  14563293  0.507121      0.013529  0.236040  0.003159  0.226270   \n",
       "3  21086297  0.613642      0.001056  0.188158  0.000059  0.193943   \n",
       "4  22982444  0.556457      0.004667  0.208896  0.004894  0.214843   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.044750  \n",
       "1       0.046111  \n",
       "2       0.013882  \n",
       "3       0.003143  \n",
       "4       0.010243  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.set_index(\"id\").to_csv('Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
